{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27523582-eecb-4348-aaf4-1aa88f75393d",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "This notebook contains exploratory analyses of defender performance model outputs and match event data.\n",
    "\n",
    "Structure:\n",
    "- Setup & data loading\n",
    "- Basic counts and error handling\n",
    "- Merge results & compute features\n",
    "- Case study: Grid/Zonal analysis\n",
    "- Defensive duos\n",
    "- Time-based rolling analysis\n",
    "- Defender-level normalisation & grouping\n",
    "- Defender ↔ actions correlation (logit/OLS)\n",
    "- xT / pitch-value experiments\n",
    "- Model evaluation (BCE, bootstrap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bf93182-de42-462d-8368-e1e3a6ee7c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Apps\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Apps\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "C:\\Apps\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "# Setup: imports and working directory\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "# plotting helper libs used later\n",
    "from mplsoccer import Pitch\n",
    "# If using seaborn or statsmodels later, import when needed\n",
    "# import seaborn as sns\n",
    "# import statsmodels.api as sm\n",
    "\n",
    "# Change to data directory (keeps your original path)\n",
    "os.chdir('PFF_2023-24_Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8692b97-2ed0-4655-9588-aec7b2087dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata and player position info\n",
    "metadata = pd.read_csv('metadata_updated.csv')\n",
    "players = pd.read_csv('players.csv')\n",
    "\n",
    "# Create mapping from player nickname to position group\n",
    "player_position_dict = dict(zip(players['nickname'], players['positionGroupType']))\n",
    "\n",
    "# Extract match ids from metadata for later iteration\n",
    "match_ids = metadata['id'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962953db",
   "metadata": {},
   "source": [
    "### Count games per team (home + away)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3038847e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_id = metadata[~metadata['id'].isin(match_id_error)]\n",
    "\n",
    "# Count home and away games per team and sum them\n",
    "home_games = metadata_id['homeTeam_name'].value_counts()\n",
    "away_games = metadata_id['awayTeam_name'].value_counts()\n",
    "total_games = home_games.add(away_games, fill_value=0).sort_values(ascending=False)\n",
    "\n",
    "print(\"Total games per team:\")\n",
    "for team, count in total_games.items():\n",
    "    print(f\"{team}: {count} games\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09e673e-6ec5-48a9-a5fd-ed57c8b7ef24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through match ids and attempt to read per-match analysis + events.\n",
    "# Saves problematic match ids into `match_id_error`.\n",
    "results_model1 = []\n",
    "events_dataframes = []\n",
    "match_id_error = []\n",
    "\n",
    "for match_id in match_ids:\n",
    "    try:\n",
    "        results_model_temp = pd.read_csv(f'results/match_{match_id}_analysis/{match_id}_defender_performance_dataframe.csv')\n",
    "        events_df = pd.read_csv(f'game_dataframes/{match_id}/{match_id}_events_df.csv')\n",
    "        events_df['match_id'] = match_id\n",
    "        results_model1.append(results_model_temp)\n",
    "        events_dataframes.append(events_df)\n",
    "    except Exception as e:\n",
    "        # print id of match that failed (and optionally the exception)\n",
    "        print(\"Error loading match:\", match_id, \"->\", e)\n",
    "        match_id_error.append(match_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3201a5b7-c990-4ac6-ab4d-2d73289e996f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all per-match dataframes into single DataFrames\n",
    "results_model1_df = pd.concat(results_model1, ignore_index=True)\n",
    "del results_model1  # free memory\n",
    "\n",
    "all_events_df = pd.concat(events_dataframes, ignore_index=True)\n",
    "\n",
    "# Compute relative performance (subtract mean Total_Perf for that frame)\n",
    "avg_performance = results_model1_df.groupby(['match_id', 'match_frame'])['Total_Perf'].transform('mean')\n",
    "results_model1_df['relative_performance'] = results_model1_df['Total_Perf'] - avg_performance\n",
    "\n",
    "# Map defender/attacker to position groups using the player_position_dict\n",
    "results_model1_df['defender_pos'] = results_model1_df['Defender'].map(player_position_dict)\n",
    "results_model1_df['attacker_pos'] = results_model1_df['Attacker'].map(player_position_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba624e73-ec0b-45ff-9995-0f2eacdd9c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick exploration cell: unique possession event types in events dataframe\n",
    "all_events_df['possessionEventType'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9a4c6b-8b9c-4ef9-b53e-a9ddba53214f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist combined results for convenience\n",
    "results_model1_df.to_csv('results_model_df.csv', index=False)\n",
    "\n",
    "# Keep only one row per (match_id, match_frame, Defender) — useful for some aggregated analyses\n",
    "results_model1_df_unique = results_model1_df.drop_duplicates(subset=['match_id', 'match_frame', 'Defender'])\n",
    "results_model1_df_unique.to_csv('results_model_df_unique.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c96be86-0f04-4260-bec1-a85be0f1ce17",
   "metadata": {},
   "source": [
    "## Zonal heatmap\n",
    "- We split the pitch into a 6x4 grid.\n",
    "- We compute means and confidence intervals per grid cell and plot a heatmap with annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f68f7b7-49a4-4108-a09e-5b9e989d954c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_grid_cell(x, y, pitch_length=105, pitch_width=68, grid_length=6, grid_width=4):\n",
    "    # Calculate the size of each grid cell\n",
    "    cell_length = pitch_length / grid_length\n",
    "    cell_width = pitch_width / grid_width\n",
    "\n",
    "    # Calculate which grid cell the coordinate falls into\n",
    "    # We use np.floor to get the integer part and add 1 to start counting from 1\n",
    "    x_cell = int(np.floor(x / cell_length)) + 1\n",
    "    y_cell = int(np.floor(y / cell_width)) + 1\n",
    "\n",
    "    # Ensure coordinates within pitch boundaries\n",
    "    x_cell = min(max(x_cell, 1), grid_length)\n",
    "    y_cell = min(max(y_cell, 1), grid_width)\n",
    "\n",
    "    # Convert to single grid cell number (left to right, top to bottom)\n",
    "    grid_cell = (y_cell - 1) * grid_length + x_cell\n",
    "\n",
    "    return grid_cell - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13b5368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare unique results and select a player (example: Mario Lemina)\n",
    "results_model1_df_unique = results_model1_df.drop_duplicates(subset=['match_id', 'match_frame', 'Defender'])\n",
    "results_model1_df_unique_lemina = results_model1_df[results_model1_df['Defender'] == 'Mario Lemina'].drop_duplicates(subset=['match_id', 'match_frame'])\n",
    "\n",
    "# Assign grid cell for Ball location and, for the selected player, for defender location\n",
    "results_model1_df_unique['grid_cell'] = results_model1_df_unique.apply(lambda row: assign_grid_cell(row['Ball_x'], row['Ball_y']), axis=1)\n",
    "results_model1_df_unique_lemina['grid_cell'] = results_model1_df_unique_lemina.apply(lambda row: assign_grid_cell(row['Def_x'], row['Def_y']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50c5a5e-8cee-4f53-b3cc-801f723d75cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence interval helper using t-distribution (returns +/- half-width)\n",
    "def confidence_interval(data, confidence=0.95):\n",
    "    if len(data) < 2:  # handle small-sample case\n",
    "        return 0\n",
    "    ci = stats.t.interval(confidence, len(data)-1, loc=data.mean(), scale=stats.sem(data))\n",
    "    return (ci[1] - ci[0]) / 2\n",
    "\n",
    "# Aggregate statistics per grid cell\n",
    "grid_stats = results_model1_df_unique.groupby('grid_cell').agg({\n",
    "    'Total_Perf': ['mean', 'count', lambda x: confidence_interval(x)]\n",
    "}).reset_index()\n",
    "grid_stats.columns = ['grid_cell', 'mean', 'count', 'ci']\n",
    "\n",
    "# Create pitch and compute grid geometry\n",
    "pitch = Pitch(pitch_type='uefa', pitch_length=105, pitch_width=68, pitch_color='white')\n",
    "fig, ax = pitch.draw(figsize=(7.5, 5))\n",
    "\n",
    "cell_length = 105 / 6\n",
    "cell_width = 68 / 4\n",
    "\n",
    "# Prepare matrices for plotting (rows = grid_height (4), cols = grid_length (6))\n",
    "performance_matrix = np.zeros((4, 6))\n",
    "ci_matrix = np.zeros((4, 6))\n",
    "count_matrix = np.zeros((4, 6))\n",
    "\n",
    "for _, row in grid_stats.iterrows():\n",
    "    grid_cell = int(row['grid_cell'])\n",
    "    y_idx = grid_cell // 6\n",
    "    x_idx = grid_cell % 6\n",
    "    performance_matrix[y_idx, x_idx] = row['mean']\n",
    "    ci_matrix[y_idx, x_idx] = row['ci']\n",
    "    count_matrix[y_idx, x_idx] = row['count']\n",
    "\n",
    "# Plot heatmap using imshow (overlay on pitch)\n",
    "hm = ax.imshow(performance_matrix, extent=[0, 105, 0, 68],\n",
    "               origin='lower', cmap='Reds', aspect='auto', alpha=0.6)\n",
    "\n",
    "# Annotate each grid cell with mean ± CI and count\n",
    "for i in range(4):\n",
    "    for j in range(6):\n",
    "        value = performance_matrix[i, j]\n",
    "        ci = ci_matrix[i, j]\n",
    "        count = int(count_matrix[i, j])\n",
    "\n",
    "        if count > 1:\n",
    "            text = f'{value:.3f}\\n±{ci:.3f}\\n(n={count})'\n",
    "        else:\n",
    "            text = f'{value:.3f}\\n(n={count})'\n",
    "\n",
    "        ax.text(j*cell_length + cell_length/2, i*cell_width + cell_width/2,\n",
    "                text, ha='center', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3947288-762d-4140-ab8e-1e774f98443e",
   "metadata": {},
   "source": [
    "## Defensive Duos\n",
    "- Identify center-back (LCB/RCB) pairs that jointly defend the same event (same match_id + frame).\n",
    "- Sum Total_Perf when both CBs are present in the event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419ab6ca-7d5b-48b2-8641-8e3a1328e045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to center-backs\n",
    "cb_data = results_model1_df_unique[results_model1_df_unique['defender_pos'].isin(['LCB', 'RCB'])]\n",
    "\n",
    "# For pairs: group by match/frame/team then find exactly-two CB events and sum their Total_Perf\n",
    "cb_duo_performance = (\n",
    "    cb_data.groupby(['match_id', 'match_frame', 'Def_team'])\n",
    "    .apply(lambda x: (\n",
    "        '+'.join(sorted(x['Defender'])),\n",
    "        x['Total_Perf'].sum() if len(x) == 2 else None\n",
    "    ))\n",
    "    .dropna()\n",
    "    .reset_index(name='CB_Pair_Perf')\n",
    ")\n",
    "\n",
    "# Split tuple into columns\n",
    "cb_duo_performance[['CB_Pair', 'Summed_Total_Perf']] = pd.DataFrame(cb_duo_performance['CB_Pair_Perf'].tolist(), index=cb_duo_performance.index)\n",
    "cb_duo_performance = cb_duo_performance.drop(columns=['CB_Pair_Perf'])\n",
    "\n",
    "# Aggregate across events and rank\n",
    "cb_duo_ranking = (\n",
    "    cb_duo_performance.groupby(['CB_Pair'])\n",
    "    .agg({'Def_team': 'first', 'Summed_Total_Perf': 'sum'})\n",
    "    .reset_index()\n",
    "    .sort_values(by='Summed_Total_Perf', ascending=False)\n",
    ")\n",
    "\n",
    "cb_duo_ranking['Rank'] = cb_duo_ranking['Summed_Total_Perf'].rank(ascending=False, method='min')\n",
    "cb_duo_ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39ed82a-4bcb-40f9-be16-13c7f9cf97d7",
   "metadata": {},
   "source": [
    "## Time-based / Rolling Average analysis\n",
    "- Produces rolling team summaries (rolling mean, std, SE), and plots selected teams vs league average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae42edc-0868-426b-96bf-2a5e7e8c46fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up memory of any intermediate large variables (original cell removed variable references)\n",
    "try:\n",
    "    del results_model1_df_VVD\n",
    "except NameError:\n",
    "    pass\n",
    "gc.collect()\n",
    "\n",
    "# Prepare time (minutes) combining periods (1st, 2nd half)\n",
    "results_model1_df['match_time_mins'] = results_model1_df['match_time'] / 60\n",
    "results_model1_df.loc[results_model1_df['period'] == 2, 'match_time_mins'] += 45\n",
    "\n",
    "# Group by event (one event per match_id + match_frame)\n",
    "grouped_df = results_model1_df.groupby(['match_id', 'match_frame']).agg(\n",
    "    {'Def_team': 'first', 'match_time_mins': 'first', 'period': 'first', 'Total_Perf': 'sum'}\n",
    ").sort_values(['period', 'match_time_mins'])\n",
    "\n",
    "grouped_df = grouped_df.sort_values(by=['Def_team', 'match_time_mins'])\n",
    "\n",
    "# Rolling window parameters (adjust to taste)\n",
    "rolling_window = 4000\n",
    "min_periods = 800\n",
    "\n",
    "# Team-level rolling calculations\n",
    "grouped_df['Rolling_Avg'] = grouped_df.groupby('Def_team')['Total_Perf'].transform(\n",
    "    lambda x: x.rolling(window=rolling_window, min_periods=min_periods).mean()\n",
    ")\n",
    "grouped_df['Rolling_Std'] = grouped_df.groupby('Def_team')['Total_Perf'].transform(\n",
    "    lambda x: x.rolling(window=rolling_window, min_periods=min_periods).std()\n",
    ")\n",
    "grouped_df['Rolling_Count'] = grouped_df.groupby('Def_team')['Total_Perf'].transform(\n",
    "    lambda x: x.rolling(window=rolling_window, min_periods=min_periods).count()\n",
    ")\n",
    "grouped_df['Rolling_SE'] = 1.96 * (grouped_df['Rolling_Std'] / np.sqrt(grouped_df['Rolling_Count']))\n",
    "\n",
    "# League-level rolling (overall)\n",
    "grouped_df = grouped_df.sort_values(by='match_time_mins')\n",
    "grouped_df['Overall_Rolling_Avg'] = grouped_df['Total_Perf'].rolling(window=20000, min_periods=10000).mean()\n",
    "grouped_df['Overall_Rolling_Std'] = grouped_df['Total_Perf'].rolling(window=20000, min_periods=10000).std()\n",
    "grouped_df['Overall_Rolling_Count'] = grouped_df['Total_Perf'].rolling(window=20000, min_periods=10000).count()\n",
    "grouped_df['Overall_Rolling_SE'] = 1.96 * (grouped_df['Overall_Rolling_Std'] / np.sqrt(grouped_df['Overall_Rolling_Count']))\n",
    "\n",
    "# Choose teams to display\n",
    "selected_teams = ['Newcastle United', 'AFC Bournemouth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a73e2c-3449-430d-b2b2-a9b163d0f06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the time series comparisons\n",
    "plt.style.reload_library()\n",
    "plt.style.use(['science', 'no-latex'])\n",
    "with plt.style.context(['science', 'no-latex', 'bright']):\n",
    "    fig = plt.figure(figsize=(8, 4))\n",
    "\n",
    "    for team in selected_teams:\n",
    "        team_data = grouped_df[grouped_df['Def_team'] == team]\n",
    "        if team == 'AFC Bournemouth':\n",
    "            lteam = 'Bournemouth'\n",
    "        elif team == 'Newcastle United':\n",
    "            lteam = 'Newcastle'\n",
    "        else:\n",
    "            lteam = team\n",
    "\n",
    "        plt.plot(team_data['match_time_mins'], team_data['Rolling_Avg'], label=lteam, alpha=0.7)\n",
    "\n",
    "        # Error shading\n",
    "        plt.fill_between(\n",
    "            team_data['match_time_mins'],\n",
    "            team_data['Rolling_Avg'] - team_data['Rolling_SE'],\n",
    "            team_data['Rolling_Avg'] + team_data['Rolling_SE'],\n",
    "            alpha=0.2\n",
    "        )\n",
    "\n",
    "    # League average\n",
    "    plt.plot(grouped_df['match_time_mins'], grouped_df['Overall_Rolling_Avg'], color='black', label='League Average')\n",
    "    plt.fill_between(\n",
    "        grouped_df['match_time_mins'],\n",
    "        grouped_df['Overall_Rolling_Avg'] - grouped_df['Overall_Rolling_SE'],\n",
    "        grouped_df['Overall_Rolling_Avg'] + grouped_df['Overall_Rolling_SE'],\n",
    "        color='gray', alpha=0.3\n",
    "    )\n",
    "\n",
    "    plt.xlabel('Match Time (minutes)', size=17)\n",
    "    plt.ylabel('Summed Team DP', size=17)\n",
    "    plt.ylim(4, 6)\n",
    "    plt.xlim(0, 95)\n",
    "    plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.25), ncol=3, framealpha=0.8, fontsize=17)\n",
    "    plt.xticks([0,10,20,30,40,50,60,70,80,90], size=17)\n",
    "    plt.yticks([4,4.5,5,5.5,6], size=17)\n",
    "    plt.grid(False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    fig.savefig('DP_over_time.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118811bd-e065-4d24-bba4-1dc7b5205425",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "bournemouth_data = grouped_df[grouped_df['Def_team'] == 'AFC Bournemouth']['Rolling_Avg'].dropna()\n",
    "newcastle_data   = grouped_df[grouped_df['Def_team'] == 'Newcastle United']['Rolling_Avg'].dropna()\n",
    "\n",
    "# Welch's t-test (unequal variances)\n",
    "t_stat, p_value = ttest_ind(bournemouth_data, newcastle_data, equal_var=False)\n",
    "\n",
    "print(f\"T-statistic: {t_stat:.4f}\")\n",
    "print(f\"P-value:     {p_value:.4e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50a05dd-a034-44a6-9d4a-99bee0905cb7",
   "metadata": {},
   "source": [
    "## Defenders: counts and normalisation\n",
    "- Build data structures to compute total games/events per team and per opponent.\n",
    "- Provide normalization functions used later.\n",
    "- Code for comparing defender influence against particular attackers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eb4294-c2e1-4493-9806-5b1a58d6bb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine number of games per defending team (counts of distinct match_id)\n",
    "total_game_df = results_model1_df.drop_duplicates(subset=['Def_team', 'match_id']).groupby('Def_team').count()\n",
    "game_count = {}\n",
    "for team, row in total_game_df.iterrows():\n",
    "    # Take any column value since they're consistent\n",
    "    count = row['Attacker']\n",
    "    game_count[team] = count\n",
    "\n",
    "# Count defending events by (Def_team, match_id, Att_team)\n",
    "defending_event_df = results_model1_df.drop_duplicates(subset=['Def_team', 'match_frame', 'match_id']).groupby(['Def_team', 'match_id', 'Att_team']).count()\n",
    "defending_event_count = {}\n",
    "for (team, match_id, att_team), row in defending_event_df.iterrows():\n",
    "    count = row['Attacker']\n",
    "    defending_event_count[(team, match_id, att_team)] = count\n",
    "\n",
    "# Count defending events aggregated by (Def_team, Att_team)\n",
    "defending_event_df = results_model1_df.drop_duplicates(subset=['Def_team', 'match_frame', 'match_id']).groupby(['Def_team', 'Att_team']).count()\n",
    "defending_attacking_count = {}\n",
    "for (team, att_team), row in defending_event_df.iterrows():\n",
    "    count = row['Attacker']\n",
    "    defending_attacking_count[(team, att_team)] = count\n",
    "\n",
    "# Build total defending event count per team (sum across matches / opponents)\n",
    "total_defending_event_count = {}\n",
    "for (team, _, _), value in defending_event_count.items():\n",
    "    if team not in total_defending_event_count:\n",
    "        total_defending_event_count[team] = 0\n",
    "    total_defending_event_count[team] += value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001eaf89-3532-4aa2-b12d-dc0278dfef67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization helper functions\n",
    "def normalize_performance(row, team_events, scale_factor=10000):\n",
    "    team = row['Def_team']\n",
    "    return (row['Total_Perf'] / team_events[team])  # keeps same scale as original (commented out multiplication)\n",
    "\n",
    "def normalize_summed_performance(row, team_events, scale_factor=10000):\n",
    "    team = row['Def_team']\n",
    "    return (row['Summed_Total_Perf'] / team_events[team])\n",
    "\n",
    "def normalize_performance_attack(row, team_events, scale_factor=1000):\n",
    "    team = row['Def_team']\n",
    "    att_team = row['Att_team']\n",
    "    return (row['Def_Influence'] / team_events[(team, att_team)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6280280-0fdc-45fb-aa19-6bc245307b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard error (95% CI approx) helper\n",
    "def sem(x):\n",
    "    return 1.96 * (x.std() / np.sqrt(len(x)))\n",
    "\n",
    "# Compute SEM per defender position (on deduped defender-frame events)\n",
    "results = (results_model1_df\n",
    "    .drop_duplicates(subset=['Defender', 'match_frame', 'match_id'])\n",
    "    .groupby(['defender_pos'])\n",
    "    .agg({'Total_Perf': sem})\n",
    ")\n",
    "\n",
    "grouped_position_values = results_model1_df.drop_duplicates(subset=['Defender', 'match_frame', 'match_id']).groupby(['defender_pos']).agg({'Total_Perf': 'mean'})\n",
    "grouped_position_sems = results_model1_df.drop_duplicates(subset=['Defender', 'match_frame', 'match_id']).groupby(['defender_pos']).agg({'Total_Perf': sem})\n",
    "\n",
    "grouped_position_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5874a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic bar chart of mean DP by position (using placeholder numbers taken from your original plot)\n",
    "plt.style.reload_library()\n",
    "plt.style.use(['science', 'no-latex'])\n",
    "with plt.style.context(['science', 'no-latex', 'bright']):\n",
    "    positions = ['Goalkeeper', 'Center Defender', 'Wide Defender','Center Midfielder','Wide Midfielder','Center Forward']\n",
    "    means = [0.0077,0.042179,0.053402,0.045113,0.042546,0.034278]\n",
    "    sems = [0.0003,0.0003,0.0004,0.0003,0.0004,0.0004]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 3.5))\n",
    "    bars = ax.bar(positions, means, yerr=sems, capsize=3)\n",
    "    ax.set_xlabel('Position', size=12)\n",
    "    ax.set_ylabel('Mean Def. Performance (DP)', size=12)\n",
    "    plt.xticks(rotation=40, ha='right', size=12)\n",
    "    plt.yticks([0.00,0.02,0.04,0.06], size=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5263e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: filter rows with maximum Def_Attention per attacker-matchframe (keeps highest attention)\n",
    "results_model1_df_max_attention = (results_model1_df\n",
    "    .sort_values('Def_Attention', ascending=False)\n",
    "    .drop_duplicates(subset=['Attacker', 'match_frame', 'match_id'], keep='first')\n",
    ")\n",
    "\n",
    "# Example selection for a specific player (original named Fabian Schär — change as needed)\n",
    "results_model1_df_DEF = results_model1_df_max_attention[results_model1_df_max_attention['Defender'] == 'Fabian Schär']\n",
    "\n",
    "# Normalize influence using defending_attacking_count mapping\n",
    "results_model1_df_DEF['Normalized_Influence'] = results_model1_df_DEF.apply(lambda row: normalize_performance_attack(row, defending_attacking_count), axis=1)\n",
    "\n",
    "results_model1_df_DEF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851963ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter attackers with more than N examples then group and rank by Def_Influence\n",
    "results_model1_df_DEF_max_attention_filtered = results_model1_df_DEF.groupby('Attacker').filter(lambda x: len(x) > 150)\n",
    "\n",
    "results_model1_df_DEF_max_attention_grouped = (\n",
    "    results_model1_df_DEF_max_attention_filtered\n",
    "      .groupby('Attacker')\n",
    "      .agg({'attacker_pos': 'first', 'Def_Influence': 'mean'})\n",
    "      .sort_values('Def_Influence', ascending=False)\n",
    ")\n",
    "\n",
    "# Show those who are CFs (example)\n",
    "results_model1_df_DEF_max_attention_grouped[results_model1_df_DEF_max_attention_grouped['attacker_pos'] == 'CF']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432c6df2",
   "metadata": {},
   "source": [
    "## xT / Pitch-value helper\n",
    "- Load an xT grid (matrix) and get the cell value for a location (x,y).\n",
    "- Note: grid shape and orientation correspond to how the xT grid was saved; keep the same logic to be consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7c3755-7f53-47b3-8bc5-7cada91eb66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xT_grid = pd.read_csv('xT_grid.csv', header=None)\n",
    "\n",
    "def get_pitch_value(data, x, y, pitch_length=105, pitch_width=68):\n",
    "    # Clamp coordinates inside pitch if necessary\n",
    "    if not (0 <= x <= pitch_length and 0 <= y <= pitch_width):\n",
    "        if x > 105:\n",
    "            x = 104\n",
    "        elif x < 0:\n",
    "            x = 1\n",
    "        if y > 68:\n",
    "            y = 67\n",
    "        elif y < 0:\n",
    "            y = 1\n",
    "\n",
    "    # Determine grid resolution\n",
    "    rows, cols = data.shape[0], data.shape[1]\n",
    "    cell_length = pitch_length / (cols)\n",
    "    cell_width = pitch_width / (rows)\n",
    "    col_index = int(x / cell_length)\n",
    "    row_index = int(y / cell_width)\n",
    "    col_index = min(col_index, cols - 1)\n",
    "    row_index = min(row_index, rows - 1)\n",
    "\n",
    "    return data.iloc[row_index, col_index]\n",
    "\n",
    "# Example lookup\n",
    "get_pitch_value(xT_grid, 104, 33)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f394f3c8",
   "metadata": {},
   "source": [
    "## Get xT of certain players in particular games \n",
    "- Used to analyse attacker performance in experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ec2c80-6a9d-4870-a19d-28c7f6663c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: select two matches and extract xT for specific player events\n",
    "game_events = all_events_df[(all_events_df['match_id'] == 13509) | (all_events_df['match_id'] == 13563)]\n",
    "player_events = game_events[game_events['player_name'] == 'Elijah Adebayo'].dropna()\n",
    "player_events['xT_value'] = player_events.apply(lambda row: get_pitch_value(xT_grid, row['x'], row['y']), axis=1)\n",
    "\n",
    "# Filter to shot-like / pass-like events\n",
    "player_events[(player_events['possessionEventType'] == 'PA') | (player_events['possessionEventType'] == 'SH') |\n",
    "              (player_events['possessionEventType'] == 'BC') | (player_events['possessionEventType'] == 'CR')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2aec7d-4c95-4781-9f85-d9e53cf76bd6",
   "metadata": {},
   "source": [
    "### Players & Team Buckets\n",
    "- Create coarse team buckets (Top 5, 6-10, 11-15, 16-20) and add a team-group column.\n",
    "- This is used later to compare defender performance against groups of opponents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bbe13e-a9a2-4617-bfb5-00e6dea54107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define team buckets (top-5, 6-10 etc.)\n",
    "zerotofive = ['Manchester City','Arsenal','Liverpool','Aston Villa','Tottenham Hotspur']\n",
    "fivetoten = ['Chelsea','Newcastle United','Manchester United','West Ham','Crystal Palace']\n",
    "tentofifteen = ['Brighton & Hove Albion','AFC Bournemouth','Fulham','Wolverhampton Wanderers','Everton']\n",
    "fifteentotwenty = ['Brentford','Nottingham Forest','Luton Town','Burnley','Sheffield United']\n",
    "\n",
    "def get_team_group(team):\n",
    "    if team in zerotofive:\n",
    "        return 'Top 5'\n",
    "    elif team in fivetoten:\n",
    "        return '6th-10th'\n",
    "    elif team in tentofifteen:\n",
    "        return '11th-15th'\n",
    "    elif team in fifteentotwenty:\n",
    "        return '16th-20th'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "results_model1_df['Att_team_group'] = results_model1_df['Att_team'].apply(get_team_group)\n",
    "\n",
    "# Group by defender and attacking team group\n",
    "grouped_defender_values = (results_model1_df\n",
    "    .drop_duplicates(subset=['Defender', 'match_frame', 'match_id'])\n",
    "    .groupby(['Defender', 'Att_team_group'])\n",
    "    .agg({\n",
    "        'Def_team': 'first',\n",
    "        'defender_pos': 'first',\n",
    "        'Total_Perf': 'sum'\n",
    "    })\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874c06d1",
   "metadata": {},
   "source": [
    "### Normalising Defender Performance vs Opponent Groups\n",
    "Functions here compute:\n",
    "- aggregated counts of defensive events against specific attacking teams,\n",
    "- normalization helpers to scale defender totals relative to the number of defensive opportunities\n",
    "\n",
    "Notes:\n",
    "- These helpers rely on `defending_attacking_count` and `total_defending_event_count` computed earlier.\n",
    "- Returns 0 when no events exist for a matchup to avoid division-by-zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bba1a5-2fab-411c-8ebf-07616f1c09fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: sum defense counts for a defending team against a list of attacking teams\n",
    "def get_defense_sum_against_group(defending_team, attacking_teams, matchup_dict):\n",
    "    total = 0\n",
    "    for att_team in attacking_teams:\n",
    "        if (defending_team, att_team) in matchup_dict:\n",
    "            total += matchup_dict[(defending_team, att_team)]\n",
    "    return total\n",
    "\n",
    "# Normalize team-performance vs group of attacking teams\n",
    "def normalize_team_performance(row):\n",
    "    defending_team = row['Def_team']\n",
    "    att_group = row['Att_team_group']\n",
    "\n",
    "    if att_group == 'Top 5':\n",
    "        att_teams = zerotofive\n",
    "    elif att_group == '6th-10th':\n",
    "        att_teams = fivetoten\n",
    "    elif att_group == '11th-15th':\n",
    "        att_teams = tentofifteen\n",
    "    else:\n",
    "        att_teams = fifteentotwenty\n",
    "\n",
    "    team_group_sum = get_defense_sum_against_group(defending_team, att_teams, defending_attacking_count)\n",
    "    if team_group_sum == 0:\n",
    "        return 0\n",
    "    return row['Total_Perf'] / team_group_sum\n",
    "\n",
    "grouped_defender_values['Normalized_Perf'] = grouped_defender_values.apply(normalize_team_performance, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d5296f",
   "metadata": {},
   "source": [
    "### Defender Leaderboards (Per-player aggregates)\n",
    "Aggregate Total_Perf per defender across events and compute normalized metrics for ranking.\n",
    "- `Normalized_Perf` divides total defensive performance by the team's defending event count.\n",
    "- Useful to compare defenders across teams with different numbers of events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0777763d-c782-48f5-9a0d-e2eb3bbf79d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-defender total performance and normalized performance\n",
    "grouped_defender_values = results_model1_df_unique.drop_duplicates(subset=['Defender', 'match_frame', 'match_id']).groupby(['Defender']).agg({'Def_team': 'first', 'defender_pos': 'first', 'Total_Perf': 'sum'})\n",
    "grouped_defender_values['Normalized_Perf'] = grouped_defender_values.apply(lambda row: normalize_performance(row, total_defending_event_count), axis=1)\n",
    "grouped_defender_values_normalized = grouped_defender_values.sort_values('Normalized_Perf')\n",
    "\n",
    "# Show a selection (example: top/last defenders among LCB/RCB)\n",
    "grouped_defender_values_normalized[((grouped_defender_values_normalized['defender_pos'] == 'LCB') | (grouped_defender_values_normalized['defender_pos'] == 'RCB'))].tail(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff0e0d1",
   "metadata": {},
   "source": [
    "### Defender ↔ Action Correlation (Computationally Intensive)\n",
    "Test whether a defender appears in subsequent events shortly after a defensive frame (e.g., within next 3 events).\n",
    "Important notes:\n",
    "- This section can be very slow on the full dataset. Use a subset or the chunked implementation.\n",
    "- Prefer the unique-combinations + dictionary mapping approach for speed and memory efficiency.\n",
    "- Progress bars (tqdm) are used to monitor progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945d7aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def check_defender_action(row, events_df):\n",
    "    # Get next 3 events for the same match\n",
    "    mask = (events_df['match_id'] == row['match_id']) & \\\n",
    "           (events_df['frameNum'] >= row['match_frame']) & \\\n",
    "           (events_df['frameNum'] <= row['match_frame'] + 3)\n",
    "    next_events = events_df[mask]\n",
    "    return (next_events['player_name'] == row['Defender']).any()\n",
    "\n",
    "def process_in_chunks(df, events_df, chunk_size=100000):\n",
    "    events_df = events_df.sort_values(['match_id', 'frameNum'])\n",
    "    results = []\n",
    "    for start_idx in tqdm(range(0, len(df), chunk_size)):\n",
    "        end_idx = min(start_idx + chunk_size, len(df))\n",
    "        chunk = df.iloc[start_idx:end_idx]\n",
    "        chunk_results = chunk.apply(lambda row: check_defender_action(row, events_df), axis=1)\n",
    "        results.extend(chunk_results)\n",
    "    return results\n",
    "\n",
    "# Add boolean column indicating if defender acted within the next 3 events (use carefully on full dataset)\n",
    "# results_model1_df['defender_action_within_3'] = process_in_chunks(results_model1_df, all_events_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b23bbda",
   "metadata": {},
   "source": [
    "#### How this check works\n",
    "1. Build unique (Defender, match_id, match_frame) combinations to avoid redundant checks.\n",
    "2. For each unique combination, look up the next 3 events in the events DataFrame.\n",
    "3. Cache results in a dictionary and map back to the main DataFrame.\n",
    "Tip: run a small subset first (e.g., first 50k rows) to make sure the logic is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81107c33-8dbd-4a7f-9571-73077d7dab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative approach that processes unique (Defender, match_id, match_frame) combos to reduce work:\n",
    "results_model1_df_subset = results_model1_df[:500000].copy()  # subset for speed while developing\n",
    "\n",
    "def check_defender_action_fast(row, events_df):\n",
    "    match_events = events_df[(events_df['match_id'] == row['match_id']) & (events_df['frameNum'] >= row['match_frame'])]\n",
    "    next_events = match_events.sort_values('frameNum').head(3)\n",
    "    return (next_events['player_name'] == row['Defender']).any()\n",
    "\n",
    "unique_combinations = results_model1_df_subset[['Defender', 'match_id', 'match_frame']].drop_duplicates()\n",
    "\n",
    "unique_results = []\n",
    "for _, row in tqdm(unique_combinations.iterrows(), total=len(unique_combinations), desc=\"Processing unique combinations\"):\n",
    "    unique_results.append(check_defender_action_fast(row, all_events_df))\n",
    "\n",
    "result_dict = dict(zip(\n",
    "    zip(unique_combinations['Defender'], unique_combinations['match_id'], unique_combinations['match_frame']),\n",
    "    unique_results\n",
    "))\n",
    "\n",
    "results_model1_df_subset['defender_action_within_3'] = results_model1_df_subset.apply(\n",
    "    lambda row: result_dict[(row['Defender'], row['match_id'], row['match_frame'])],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8751e2d7-2af6-4684-a2a6-18b5a44ce7d6",
   "metadata": {},
   "source": [
    "### Logistic Regression: Does relative performance predict the defender acting soon after?\n",
    "Fit a simple logistic regression (logit) to test whether `relative_performance` predicts `defender_action_within_3`.\n",
    "\n",
    "Notes:\n",
    "- Ensure the boolean column `defender_action_within_3` exists and is binary (0/1).\n",
    "- Use a subset when prototyping, then scale up if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85dd4c5-9b4d-43f0-88ce-9cf57bc5d40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logit model: does relative performance predict whether defender acts within next 3 events?\n",
    "import statsmodels.api as sm\n",
    "\n",
    "results_model1_df_subset = results_model1_df_subset.dropna(subset=['relative_performance', 'defender_action_within_3'])\n",
    "X = results_model1_df_subset['relative_performance']\n",
    "y = results_model1_df_subset['defender_action_within_3'].astype(int)\n",
    "\n",
    "X = sm.add_constant(X)\n",
    "logit_model = sm.Logit(y, X).fit(disp=False)\n",
    "print(logit_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd297e3",
   "metadata": {},
   "source": [
    "### OLS: Defender Attention → Defender Influence\n",
    "Estimate linear relationship between `Def_Attention` (predictor) and `Def_Influence` (response).\n",
    "- Uses ordinary least squares (statsmodels OLS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39387e9d-5234-4593-a395-4c5788c00c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression: Def_Attention predicting Def_Influence\n",
    "X = results_model1_df['Def_Attention'].dropna()\n",
    "y = results_model1_df['Def_Influence'].dropna()\n",
    "# Align indices\n",
    "df_xy = pd.concat([X, y], axis=1).dropna()\n",
    "X = sm.add_constant(df_xy['Def_Attention'])\n",
    "y = df_xy['Def_Influence']\n",
    "\n",
    "ols_model = sm.OLS(y, X).fit()\n",
    "print(ols_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29b2321",
   "metadata": {},
   "source": [
    "### Attention Binning & Stratified Sampling\n",
    "- Bin `Def_Attention` into equal-width bins and draw stratified samples for plotting and robust visual estimation.\n",
    "- Ensures a balanced representation across the attention range for clearer scatter/regression visuals.\n",
    "Notes:\n",
    "- Adjust `n_bins` and `n_per_bin` depending on data size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce22bae2-0b2f-4059-b00d-0209b52a9c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read saved combined results if needed\n",
    "# results_model1_df = pd.read_csv('results_model1_df.csv')\n",
    "\n",
    "# Binning attention to create stratified sample\n",
    "n_bins = 20\n",
    "n_per_bin = 100\n",
    "\n",
    "min_attention = results_model1_df['Def_Attention'].min()\n",
    "max_attention = results_model1_df['Def_Attention'].max()\n",
    "bin_edges = np.linspace(min_attention, max_attention, n_bins + 1)\n",
    "results_model1_df['attention_bin'] = pd.cut(results_model1_df['Def_Attention'], bins=bin_edges, labels=False)\n",
    "\n",
    "# Stratified sampling across bins\n",
    "stratified_sample = pd.DataFrame()\n",
    "for bin_num in range(n_bins):\n",
    "    bin_data = results_model1_df[results_model1_df['attention_bin'] == bin_num]\n",
    "    if len(bin_data) >= n_per_bin:\n",
    "        sampled_bin = bin_data.sample(n=n_per_bin, random_state=42)\n",
    "        stratified_sample = pd.concat([stratified_sample, sampled_bin])\n",
    "    else:\n",
    "        stratified_sample = pd.concat([stratified_sample, bin_data])\n",
    "\n",
    "X_sample = stratified_sample['Def_Attention']\n",
    "y_sample = stratified_sample['Def_Rel_Influence (%)']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848a612a",
   "metadata": {},
   "source": [
    "### Visualization: Def_Attention vs Relative Influence\n",
    "Scatter plot with regression line and 95% confidence bands.\n",
    "- Uses model coefficients (or estimate them in the notebook) to draw the line.\n",
    "- Confidence intervals can be drawn using standard errors from the fitted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b840470e-7e3b-4e8a-833b-725ff5820411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter + regression line with CI (using fixed slope/intercept from prior model)\n",
    "plt.style.reload_library()\n",
    "plt.style.use(['science', 'no-latex'])\n",
    "with plt.style.context(['science', 'no-latex', 'bright']):\n",
    "    fig = plt.figure(figsize=(7, 4))\n",
    "    plt.scatter(X_sample, y_sample, alpha=0.5, color='blue', label='Data points')\n",
    "\n",
    "    # Regression line & CI (example coefficients)\n",
    "    x_line = np.linspace(0, 0.48, 100)\n",
    "    y_line = 4.9005 + 69.316 * x_line\n",
    "\n",
    "    std_err_intercept = 0.002\n",
    "    std_err_slope = 0.035\n",
    "    ci = 1.96\n",
    "    y_err = ci * np.sqrt(std_err_intercept**2 + (x_line**2 * std_err_slope**2))\n",
    "\n",
    "    plt.plot(x_line, y_line, color='red', label='Regression line', linewidth=3)\n",
    "    plt.fill_between(x_line, y_line - y_err, y_line + y_err, color='red', alpha=0.2, label='95% CI')\n",
    "\n",
    "    plt.xlabel('Defender Attention', size=15)\n",
    "    plt.ylabel('Relative Defender Influence (%)', size=15)\n",
    "    plt.xticks(size=15)\n",
    "    plt.yticks(size=15)\n",
    "    plt.ylim(-1, 110)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    fig.savefig('def_att_corr.png', dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b5368b",
   "metadata": {},
   "source": [
    "### Model Evaluation: BCE, AUC, F1\n",
    "Compute binary cross-entropy (BCE), AUC, and F1 for different prediction variants.\n",
    "- Ensure the required probability columns exist (e.g., `original_prob`, `prob_without_top`, ...).\n",
    "- Used for attention experiments.\n",
    "- These metrics give complementary perspectives: BCE (calibration), AUC (discrimination), F1 (classification at threshold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb70bdbc-5ab2-4e7f-aa04-db02ffaf3e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss, roc_auc_score, f1_score\n",
    "\n",
    "def calculate_metrics(y_true, y_pred_proba, threshold=0.5):\n",
    "    bce = log_loss(y_true, y_pred_proba)\n",
    "    auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    y_pred = (y_pred_proba > threshold).astype(int)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    return bce, auc, f1\n",
    "\n",
    "# Example columns to evaluate; ensure columns exist in results_model1_df\n",
    "columns_to_evaluate = ['original_prob', 'prob_without_top', 'prob_without_random', 'prob_without_bottom']\n",
    "true_labels = results_model1_df['is_receiver']\n",
    "\n",
    "results = {}\n",
    "for col in columns_to_evaluate:\n",
    "    bce, auc, f1 = calculate_metrics(true_labels, results_model1_df[col])\n",
    "    results[col] = {'BCE Loss': bce, 'AUC': auc, 'F1 Score': f1}\n",
    "\n",
    "for model, metrics in results.items():\n",
    "    print(f\"\\n{model}:\")\n",
    "    for metric_name, value in metrics.items():\n",
    "        print(f\"{metric_name}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64390fd4",
   "metadata": {},
   "source": [
    "### Bootstrap comparison of BCE differences\n",
    "Use bootstrap resampling to estimate mean difference in BCE between the baseline model and alternatives.\n",
    "- Returns mean difference and a 95% bootstrap CI.\n",
    "- Increase `n_iterations` for tighter CI estimates (1000+ recommended for final results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5802f502-22b5-417c-9031-51936c414363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def bootstrap_bce_differences(y_true, y_pred_orig, y_pred_compare, n_iterations=100):\n",
    "    n_samples = len(y_true)\n",
    "    bce_differences = []\n",
    "\n",
    "    for i in range(n_iterations):\n",
    "        indices = np.random.randint(0, n_samples, n_samples)\n",
    "        y_true_boot = y_true[indices]\n",
    "        y_pred_orig_boot = y_pred_orig[indices]\n",
    "        y_pred_compare_boot = y_pred_compare[indices]\n",
    "        bce_orig = log_loss(y_true_boot, y_pred_orig_boot)\n",
    "        bce_compare = log_loss(y_true_boot, y_pred_compare_boot)\n",
    "        bce_differences.append(bce_compare - bce_orig)\n",
    "\n",
    "    mean_diff = np.mean(bce_differences)\n",
    "    ci = np.percentile(bce_differences, [2.5, 97.5])\n",
    "    return mean_diff, ci\n",
    "\n",
    "true_labels = results_model1_df['is_receiver'].values\n",
    "original_probs = results_model1_df['original_prob'].values\n",
    "columns_to_compare = ['prob_without_top', 'prob_without_random', 'prob_without_bottom']\n",
    "\n",
    "results = {}\n",
    "for col in columns_to_compare:\n",
    "    compare_probs = results_model1_df[col].values\n",
    "    mean_diff, ci = bootstrap_bce_differences(true_labels, original_probs, compare_probs)\n",
    "    results[col] = {'Mean BCE Difference': mean_diff, 'CI': ci}\n",
    "\n",
    "for model, metrics in results.items():\n",
    "    print(f\"\\n{model}:\")\n",
    "    print(f\"Mean BCE Difference: {metrics['Mean BCE Difference']:.4f}\")\n",
    "    print(f\"95% CI: [{metrics['CI'][0]:.4f}, {metrics['CI'][1]:.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c0715f",
   "metadata": {},
   "source": [
    "### Fidelity Plot: Masking experiments (Top-K / Random-K / Bottom-K)\n",
    "Visualise how BCE increases as we mask more nodes for different masking strategies.\n",
    "- Bands indicate variability\n",
    "- Useful to demonstrate model sensitivity to targeted vs random node removal.\n",
    "- Values are real and extracted from the values computed in previous cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e2abc3-ea2d-4959-bf35-837492cdcc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate the original figure showing increase in BCE loss when masking nodes\n",
    "plt.style.reload_library()\n",
    "plt.style.use(['science', 'no-latex'])\n",
    "with plt.style.context(['science', 'no-latex', 'bright']):\n",
    "    error1 = np.array([0.0000,0.0001,0.0001,0.0001])\n",
    "    error2 = np.array([0.0000,0.0001,0.0001,0.0001])\n",
    "    error3 = np.array([0.0000,0.0001,0.0001,0.0001])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    ax.fill_between([0,1,2,3], np.array([0,0.0058,0.0096,0.0132]) - error1, np.array([0,0.0058,0.0096,0.0132]) + error1, alpha=0.2, color='blue')\n",
    "    ax.fill_between([0,1,2,3], np.array([0,0.0018,0.0038,0.0063]) - error2, np.array([0,0.0018,0.0038,0.0063]) + error2, alpha=0.2, color='red')\n",
    "    ax.fill_between([0,1,2,3], np.array([0,0.0005,0.0013,0.0024]) - error3, np.array([0,0.0005,0.0013,0.0024]) + error3, alpha=0.2, color='green')\n",
    "\n",
    "    ax.plot([0,1,2,3], [0,0.0058,0.0096,0.0132], label='Top-K', color='blue')\n",
    "    ax.plot([0,1,2,3], [0,0.0018,0.0038,0.0063], label='Random-K', color='red')\n",
    "    ax.plot([0,1,2,3], [0,0.0005,0.0013,0.0024], label='Bottom-K', color='green')\n",
    "\n",
    "    ax.set_xlabel('Number of Masked Nodes', size=20)\n",
    "    ax.set_ylabel('Increase in BCE Loss', size=20)\n",
    "    ax.legend(fontsize=20)\n",
    "    plt.xticks([0,1,2,3], size=20)\n",
    "    plt.yticks([0.000,0.002,0.004,0.006,0.008,0.010,0.012,0.014], size=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    fig.savefig('fidelity_plot.png', dpi=800, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448a9bac",
   "metadata": {},
   "source": [
    "### Experiment 3: Next-ball Pitch-value & Defensive Impact (xT)\n",
    "Test whether defensive performance in an event correlates with subsequent change in pitch value (xT) between the current ball location and the next ball location.\n",
    "\n",
    "Pipeline:\n",
    "1. Build a reference DataFrame with next-ball coordinates (shifted per match).\n",
    "2. Merge back to events and aggregate Total_Perf by match/frame.\n",
    "3. Compute current and next xT and the change in value.\n",
    "4. Correlate and fit OLS to quantify the association.\n",
    "\n",
    "Notes:\n",
    "- `xT_grid` orientation must match the indexing used in get_pitch_value.\n",
    "- Use filtering to restrict to relevant ball-value ranges (e.g., exclude out-of-play)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2c6dc6-3e74-4c29-804e-59f98a74d3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up previous reference (if present)\n",
    "try:\n",
    "    del reference_df\n",
    "except NameError:\n",
    "    pass\n",
    "gc.collect()\n",
    "\n",
    "# Build reference dataframe of unique ball locations per match/frame\n",
    "reference_df = results_model1_df.drop_duplicates(subset=['match_id', 'match_frame'])[['match_id', 'match_frame', 'Ball_x', 'Ball_y']]\n",
    "reference_df = reference_df.sort_values(['match_id', 'match_frame'])\n",
    "\n",
    "# Shift within each match to get next-ball coordinates (next frame)\n",
    "reference_df['next_ball_x'] = reference_df.groupby('match_id')['Ball_x'].shift(-1)\n",
    "reference_df['next_ball_y'] = reference_df.groupby('match_id')['Ball_y'].shift(-1)\n",
    "\n",
    "# Merge shifted coordinates back to the full dataframe\n",
    "df_with_next = results_model1_df.merge(\n",
    "    reference_df[['match_id', 'match_frame', 'next_ball_x', 'next_ball_y']],\n",
    "    on=['match_id', 'match_frame'],\n",
    "    how='left'\n",
    ")\n",
    "df_with_next['next_ball_x'] = df_with_next['next_ball_x'].fillna(df_with_next['Ball_x'])\n",
    "df_with_next['next_ball_y'] = df_with_next['next_ball_y'].fillna(df_with_next['Ball_y'])\n",
    "\n",
    "# Aggregate per event (match_id/match_frame)\n",
    "grouped_data = df_with_next.groupby(['match_id', 'match_frame']).agg({\n",
    "    'Def_team': 'first', 'Ball_x': 'first', 'Ball_y': 'first',\n",
    "    'next_ball_x': 'first', 'next_ball_y': 'first', 'Total_Perf': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Normalize per 11 (like per-team average, same as original)\n",
    "grouped_data['Total_Perf'] = grouped_data['Total_Perf'] / 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43e62be-0e84-470a-8ec7-3f72868e6a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load xT grid and compute value at current and next ball location\n",
    "xT_grid = pd.read_csv('xT_grid.csv', header=None)\n",
    "\n",
    "# reuse get_pitch_value defined earlier; if necessary, redefine or ensure it is in scope\n",
    "def add_value_columns(df, value_data):\n",
    "    df['ball_loc_value'] = 0.0\n",
    "    df['next_ball_loc_value'] = 0.0\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        df.at[idx, 'ball_loc_value'] = get_pitch_value(value_data, row['Ball_x'], row['Ball_y'])\n",
    "        df.at[idx, 'next_ball_loc_value'] = get_pitch_value(value_data, row['next_ball_x'], row['next_ball_y'])\n",
    "    return df\n",
    "\n",
    "grouped_data = add_value_columns(grouped_data, xT_grid * 100)\n",
    "grouped_data['change_in_value'] = grouped_data['next_ball_loc_value'] - grouped_data['ball_loc_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec49fa83-9ca8-4904-a213-b9c9233996e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick checks: mean Total_Perf for positive value changes\n",
    "grouped_data[grouped_data['change_in_value'] > 0]['Total_Perf'].mean()\n",
    "\n",
    "# Correlation between Total_Perf and change_in_value\n",
    "grouped_data['Total_Perf'].corr(grouped_data['change_in_value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208b2ddb-7514-4052-b40f-a89446f87a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to a band of ball values and re-check means and correlation\n",
    "grouped_data_filtered = grouped_data[(grouped_data['ball_loc_value'] > 0) & (grouped_data['ball_loc_value'] < 5)]\n",
    "grouped_data_filtered[grouped_data_filtered['change_in_value'] > 0]['Total_Perf'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0f8602",
   "metadata": {},
   "source": [
    "#### Interpreting the experiment\n",
    "- The correlation shows linear association; OLS coefficients indicate expected change in xT for unit change in Total_Perf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2624221-c085-4d7d-bac2-cecb41a6662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS: does Total_Perf predict change_in_value?\n",
    "X = grouped_data_filtered['Total_Perf']\n",
    "y = grouped_data_filtered['change_in_value']\n",
    "X = sm.add_constant(X)\n",
    "model = sm.OLS(y, X).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c932537-7ad1-432b-bdf7-897a61f29fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter + regression line + CI plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(grouped_data_filtered['Total_Perf'], grouped_data_filtered['change_in_value'], alpha=0.1, color='blue', label='Data points')\n",
    "\n",
    "x_line = np.linspace(grouped_data_filtered['Total_Perf'].min(), grouped_data_filtered['Total_Perf'].max(), 100)\n",
    "y_line = model.params[0] + model.params[1] * x_line\n",
    "\n",
    "# Compute prediction intervals on x_line\n",
    "X_line = sm.add_constant(x_line)\n",
    "y_pred = model.get_prediction(X_line)\n",
    "y_err = y_pred.conf_int()\n",
    "\n",
    "plt.plot(x_line, y_line, color='red', linewidth=2, label='Regression line')\n",
    "plt.fill_between(x_line, y_err[:, 0], y_err[:, 1], color='red', alpha=0.1, label='95% CI')\n",
    "\n",
    "plt.xlabel('Total Performance')\n",
    "plt.ylabel('Change in Value')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "equation = f'y = {model.params[0]:.3f} + {model.params[1]:.3f}x'\n",
    "plt.text(0.05, 0.95, equation, transform=plt.gca().transAxes, bbox=dict(facecolor='white', alpha=0.8))\n",
    "plt.xlim(0, 5)\n",
    "plt.ylim(-5, 10)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
